{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class_system = 'IPC4'\n",
    "# n_controls = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# n_years = 'cumulative'\n",
    "# if n_years is None or n_years=='all' or n_years=='cumulative':\n",
    "#     n_years_label = ''\n",
    "# else:\n",
    "#     n_years_label = '%i_years_'%n_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output_citations = 'class_relatedness_networks_citations'\n",
    "# output_cooccurrence = 'class_relatedness_networks_cooccurrence'\n",
    "# combine_outputs = True\n",
    "\n",
    "# cooccurrence_base_file_name = 'synthetic_control_cooccurrence_'+n_years_label+'%s_preserve_years_%s'\n",
    "citations_base_file_name = 'synthetic_control_citations_'+n_years_label+'%s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_directory = '../data/'\n",
    "\n",
    "citations_controls_directory = data_directory+'Class_Relatedness_Networks/citations/controls/%s/'%class_system\n",
    "coocurrence_controls_directory = data_directory+'Class_Relatedness_Networks/cooccurrence/controls/%s/'%class_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def running_stats(df_name,\n",
    "                  file_name,\n",
    "                  controls_directory=citations_controls_directory,\n",
    "                  n_controls=n_controls,\n",
    "                 ):\n",
    "    M = None\n",
    "    all_max = None\n",
    "    all_min = None\n",
    "    t = time()\n",
    "    for randomization_id in range(n_controls):\n",
    "\n",
    "        if not randomization_id%100:\n",
    "            print(randomization_id)\n",
    "            print(\"%.0f seconds\"%(time()-t))\n",
    "            t = time()\n",
    "        \n",
    "        f = '%s_%i.h5'%(file_name, randomization_id)\n",
    "        try:\n",
    "            x = pd.read_hdf(controls_directory+f, df_name)\n",
    "        except:\n",
    "            print(\"Data not loading for %s. Continuing.\"%f)\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if M is None:\n",
    "            M = x\n",
    "            S = 0\n",
    "            all_max = M\n",
    "            all_min = M\n",
    "            continue\n",
    "        k = randomization_id+1\n",
    "        M_previous = M\n",
    "        M = M_previous.add( x.subtract(M_previous)/k )\n",
    "        S = ( x.subtract(M_previous).multiply( x.subtract(M) ) ).add(S)\n",
    "        all_max = maximum(all_max, M)\n",
    "        all_min = minimum(all_min, M)\n",
    "        gc.collect()  \n",
    "    standard_deviation = sqrt(S/(k-1))\n",
    "\n",
    "    return M, standard_deviation, all_max, all_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if output_citations:\n",
    "    M, standard_deviation, all_max, all_min = running_stats('synthetic_citations_%s'%class_system,\n",
    "                                      citations_base_file_name%class_system,\n",
    "                                      citations_controls_directory\n",
    "                                     )\n",
    "\n",
    "    store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/citations/%s.h5'%(output_citations),\n",
    "                        mode='a', table=True)\n",
    "    store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    z_scores = store['empirical_citations_%s%s'%(n_years_label, class_system)].ix[M.labels].subtract(M).divide(standard_deviation)\n",
    "\n",
    "    z_scores.values[where(z_scores==inf)]=nan \n",
    "    #All the cases where the z-scores are inf is where the 1,000 randomized controls said there should be 0 deviation, BUT\n",
    "    #the empirical case was different anyway. In each of these cases, the empirical case was JUST slightly off. Sometimes\n",
    "    #a floating point error, and sometimes off by 1 (the minimal amount for citation counts). We shall treat this as not actually\n",
    "    #deviating, and so it becomes 0/0, which is equal to nan.\n",
    "\n",
    "    store.put('/empirical_citations_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "\n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "Data not loading for synthetic_control_citations_IPC4_1.h5. Continuing.\n",
      "2\n",
      "40 seconds\n"
     ]
    }
   ],
   "source": [
    "if output_cooccurrence:\n",
    "    M = None\n",
    "    for entity in ['Firm', 'Country', 'Inventor', 'PID']:\n",
    "        (M_entity, \n",
    "         standard_deviation_entity, \n",
    "         all_max_entity, \n",
    "         all_min_entity) = running_stats('synthetic_cooccurrence_%s_%s'%(entity, class_system),\n",
    "                                          cooccurrence_base_file_name%(entity, class_system),\n",
    "                                          coocurrence_controls_directory\n",
    "                                         )\n",
    "        if M is None:\n",
    "            M = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: M_entity})\n",
    "            standard_deviation = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: standard_deviation_entity})\n",
    "            all_max = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: all_max_entity}) \n",
    "            all_min = pd.Panel4D({'Class_CoOccurrence_Count_%s'%entity: all_min_entity})\n",
    "        else:\n",
    "            M['Class_CoOccurrence_Count_%s'%entity] = M_entity\n",
    "            standard_deviation['Class_CoOccurrence_Count_%s'%entity] = standard_deviation_entity\n",
    "            all_max['Class_CoOccurrence_Count_%s'%entity] = all_max_entity\n",
    "            all_min['Class_CoOccurrence_Count_%s'%entity] = all_min_entity\n",
    "\n",
    "    store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/cooccurrence/%s.h5'%(output_cooccurrence),\n",
    "                        mode='a', table=True)\n",
    "    store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    try:\n",
    "        z_scores = store['empirical_cooccurrence_%s%s'%(n_years_label, class_system)].ix[M.labels].subtract(M).divide(standard_deviation)\n",
    "\n",
    "        z_scores.values[where(z_scores==inf)]=nan \n",
    "        #All the cases where the z-scores are inf is where the 1,000 randomized controls said there should be 0 deviation, BUT\n",
    "        #the empirical case was different anyway. In each of these cases, the empirical case was JUST slightly off. Sometimes\n",
    "        #a floating point error, and sometimes off by 1 (the minimal amount for citation counts). We shall treat this as not actually\n",
    "        #deviating, and so it becomes 0/0, which is equal to nan.\n",
    "\n",
    "        store.put('/empirical_cooccurrence_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "    except KeyError:\n",
    "        print(\"No empirical data saved to calculate z-scores with\")\n",
    "        pass\n",
    "        \n",
    "    store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if combine_outputs:\n",
    "    \n",
    "    citation_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/citations/class_relatedness_networks_citations.h5')\n",
    "    cooccurrence_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/cooccurrence/class_relatedness_networks_cooccurrence.h5')\n",
    "    \n",
    "    M = citation_store['/randomized_mean_%s%s'%(n_years_label, class_system)]\n",
    "    standard_deviation = citation_store['/randomized_std_%s%s'%(n_years_label, class_system)]\n",
    "    all_max = citation_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    all_min = citation_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    z_scores = citation_store['/empirical_citations_z_scores_%s%s'%(n_years_label, class_system)]\n",
    "    \n",
    "    M_c = cooccurrence_store['/randomized_mean_%s%s'%(n_years_label, class_system)]\n",
    "    standard_deviation_c = cooccurrence_store['/randomized_std_%s%s'%(n_years_label, class_system)]\n",
    "    all_max_c = cooccurrence_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    all_min_c = cooccurrence_store['/randomized_max_%s%s'%(n_years_label, class_system)]\n",
    "    z_scores_c = cooccurrence_store['/empirical_cooccurrence_z_scores_%s%s'%(n_years_label, class_system)]\n",
    "    \n",
    "    for label in M_c.labels:\n",
    "        M[label] = M_c[label]\n",
    "        standard_deviation[label] = standard_deviation_c[label]\n",
    "        all_max[label] = all_max_c[label]\n",
    "        all_min[label] = all_min_c[label]\n",
    "        z_scores[label] = z_scores_c[label]\n",
    "        \n",
    "    \n",
    "    combine_store = pd.HDFStore(data_directory+'Class_Relatedness_Networks/class_relatedness_networks.h5', \n",
    "                                mode='a', table=True)\n",
    "   \n",
    "    combine_store.put('/randomized_mean_%s%s'%(n_years_label, class_system), M, 'table', append=False)\n",
    "    combine_store.put('/randomized_std_%s%s'%(n_years_label, class_system), standard_deviation, 'table', append=False)\n",
    "\n",
    "    combine_store.put('/randomized_max_%s%s'%(n_years_label, class_system), all_max, 'table', append=False)\n",
    "    combine_store.put('/randomized_min_%s%s'%(n_years_label, class_system), all_min, 'table', append=False)\n",
    "\n",
    "    combine_store.put('/empirical_z_scores_%s%s'%(n_years_label, class_system), z_scores, 'table', append=False)\n",
    "\n",
    "    combine_store.close()"
   ]
  }
 ],
 "metadata": {
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
